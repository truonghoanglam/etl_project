{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "833221db-8a04-44f3-b278-0c8d563c63be",
   "metadata": {},
   "source": [
    "# I - Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d03c8a1-f6d7-4ca8-9690-6bab07fbfe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google sheet\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "# API, Dataframe\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Psyhopyg\n",
    "import psycopg2 as ps\n",
    "from psycopg2 import connect\n",
    "\n",
    "# Prefect\n",
    "from prefect import flow, task\n",
    "\n",
    "# Time\n",
    "from datetime import date, timedelta\n",
    "from time import gmtime, strftime\n",
    "\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066bb785-a19f-46de-b830-99d71143a30b",
   "metadata": {},
   "source": [
    "#\n",
    "# II - Function to connect and edit Google sheet through Google sheet API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcd73643-35b0-4a60-9ff2-26d6a4dfab4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsheet_id_api_list          = ''\n",
    "\n",
    "scope = ['https://www.googleapis.com/auth/spreadsheets',\n",
    "         \"https://www.googleapis.com/auth/drive\"]\n",
    "\n",
    "credentials = ServiceAccountCredentials.from_json_keyfile_name(\"pfg_sheet_credentials.json\", scope)\n",
    "gc = gspread.authorize(credentials)\n",
    "\n",
    "# Create sheet function\n",
    "def extract_sheet_data(destination_database_url, destination_sheet):\n",
    "    # open sheet and extract all data\n",
    "    wks = gc.open_by_key(destination_database_url).worksheet(destination_sheet)\n",
    "    data = wks.get_all_records()\n",
    "    return data\n",
    "\n",
    "def remove_sheet_data(destination_database_url, destination_sheet):\n",
    "    wks = gc.open_by_key(destination_database_url).worksheet(destination_sheet)\n",
    "    wks.batch_clear(['A:Z'])\n",
    "    # wait before move to next task\n",
    "    time.sleep(5)\n",
    "    \n",
    "def insert_all_sheet_data(data_to_load, destination_database_url, destination_sheet):\n",
    "    wks = gc.open_by_key(destination_database_url).worksheet(destination_sheet)\n",
    "    wks.update('A1', [data_to_load.columns.values.tolist()] + data_to_load.values.tolist())\n",
    "    # wait before move to next task\n",
    "    time.sleep(1 * 60)\n",
    "    \n",
    "#-----------\n",
    "\n",
    "def convert_time(x):\n",
    "    convert_time = []\n",
    "    for i in x:\n",
    "        if i != '':\n",
    "            i = i.split('T', 1)[0] +' '+ i.split('T', 1)[1]\n",
    "            i = pd.to_datetime(\n",
    "            \n",
    "                    i[:19]\n",
    "            \n",
    "                ) + pd.DateOffset(hours=7)\n",
    "            convert_time.append(str(i))\n",
    "        else:\n",
    "            convert_time.append('')\n",
    "    return convert_time\n",
    "\n",
    "# funtion to calculate time duration':\n",
    "# https://www.statology.org/pandas-convert-timedelta-to-int/\n",
    "def duration(end_time, start_time, unit):\n",
    "    duration_list = []\n",
    "    for i, e in zip(end_time, start_time):\n",
    "\n",
    "        if i != '' and e != '':\n",
    "            duration = pd.to_datetime(i) - pd.to_datetime(e)\n",
    "            if unit    == 'day':\n",
    "                duration_list.append(round(duration / pd.Timedelta(days=1), 1) )\n",
    "\n",
    "            elif unit  == 'hour':\n",
    "                duration_list.append(round(duration / pd.Timedelta(hours=1), 1) )\n",
    "\n",
    "            elif unit  == 'minute':\n",
    "                duration_list.append(round(duration / pd.Timedelta(minutes=1), 1) )\n",
    "        else:\n",
    "            duration_list.append(float())\n",
    "    return duration_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f40c56-76b0-4366-9e8f-44663a38a508",
   "metadata": {},
   "source": [
    "#\n",
    "# II - Function to connect Postgres database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6ce801c-0206-4f7c-8fe8-8ce365fb8396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kết nối với database\n",
    "def connect_to_db(host_name, dbname, port, username, password):\n",
    "    try:\n",
    "        # https://stackoverflow.com/questions/26741175/psycopg2-db-connection-hangs-on-lost-network-connection\n",
    "        conn = connect(host = host_name, database = dbname, \n",
    "               user = username, password = password, port = port, \n",
    "                       connect_timeout=3, keepalives=1, keepalives_idle=5, keepalives_interval=2, keepalives_count=2)\n",
    "    except ps.OperationalError as e:\n",
    "        raise e\n",
    "    else:\n",
    "        print('Hurrayyyyyy! I am on the CLOUD!')\n",
    "        return conn\n",
    "\n",
    "# giờ t sẽ load lên database\n",
    "host_name          = ''\n",
    "dbname             = ''\n",
    "port               = ''\n",
    "username           = ''\n",
    "password           = ''\n",
    "conn               = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c598759-749b-4c12-bbc9-38bd30b76295",
   "metadata": {},
   "source": [
    "#\n",
    "# III - ETL pipeline to extract data from API and load it to database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdaa839-9fe5-4264-b5c9-145083f6cced",
   "metadata": {},
   "source": [
    "## 1 - Extract API key list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1993069a-90c7-4bc8-9270-ef03d0582e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@task(name                = \"Step 1: Extract client list\",\n",
    "      retries             = 10, \n",
    "      retry_delay_seconds = 60)\n",
    "def s1_extract_client_list():\n",
    "    \n",
    "    # Create client list table \n",
    "    sheet_name  = ['philippines', 'thailand']\n",
    "\n",
    "    client_list = pd.DataFrame(columns = ['Client ID', 'Market', 'BD', 'Client Type', 'Client Code', 'Client Abb. Name', 'Client POS Link', 'Start date', 'API key'])\n",
    "\n",
    "    for tab in sheet_name:\n",
    "        data          = extract_sheet_data(gsheet_id_api_list, tab)\n",
    "        sheet_df      = pd.DataFrame(data)\n",
    "        sheet_df      = sheet_df.assign(Market = tab)\n",
    "        \n",
    "        # Append data into client_list  https://pandas.pydata.org/docs/user_guide/merging.html\n",
    "        client_list   = pd.concat([client_list, sheet_df])\n",
    "\n",
    "    # Filter clients that have API key\n",
    "    client_list  = client_list[['Client ID', 'Client Type', 'API key', 'Client Abb. Name', 'Market', 'Client POS Link']]\n",
    "    client_list  = client_list[client_list['API key'] != '']\n",
    "\n",
    "    # Remove 'S' in Client ID column:\n",
    "    client_id    = []\n",
    "    for i in client_list['Client ID']:\n",
    "        client_id.append(i.split('S', 2)[1])\n",
    "    client_list['Client ID'] = client_id\n",
    "\n",
    "    # Rename column\n",
    "    client_list.columns = ['shop_id', 'client_type', 'api_key', 'shop_name', 'country', 'pos_link']\n",
    "    \n",
    "    return client_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffcc92b-669a-43e7-a2f4-08b6ca0daaf5",
   "metadata": {},
   "source": [
    "## 2 - Extract orders data from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caa98776-1b4e-4f73-b547-bba7371da297",
   "metadata": {},
   "outputs": [],
   "source": [
    "@task(name                = \"Step 2: Extract orders API data\",\n",
    "      retries             = 10, \n",
    "      retry_delay_seconds = 60)\n",
    "def s2_extract_orders_api_data(client_list):   \n",
    "    # Extract data from API- \n",
    "\n",
    "    thirty_days_ago   = (date.today() + timedelta(days= -45)).strftime(\"%Y-%m-%d\")\n",
    "    yesterday         = (date.today() + timedelta(days= -1 )).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # https://www.geeksforgeeks.org/how-to-convert-datetime-to-unix-timestamp-in-python/\n",
    "    a          = datetime.date(int(thirty_days_ago[0:4]), int(thirty_days_ago[5:7]), int(thirty_days_ago[8:10]))\n",
    "    start_time = str(time.mktime(a.timetuple()))[0:-2]\n",
    "    \n",
    "    \n",
    "    #a          = datetime.date(int(yesterday[0:4]), int(yesterday[5:7]), int(yesterday[8:10]))\n",
    "    #start_time = str(time.mktime(a.timetuple()))[0:-2]\n",
    "    \n",
    "    page_size  = 1000\n",
    "    page_num   = 1\n",
    "\n",
    "    # Variable to store data:\n",
    "    data_need_order  = [] \n",
    "\n",
    "    #___________________________________________________\n",
    "\n",
    "    # Loop through each client's API and retreive data:\n",
    "    for shop_id, api_key, shop_name in zip(client_list['shop_id'], client_list['api_key'], client_list['shop_name']):\n",
    "\n",
    "        # Keep looping and get all the data inside!\n",
    "        print('Client: ' + shop_name)\n",
    "\n",
    "        # order data\n",
    "        while True:\n",
    "            url_order   = f'https://pos.pages.fm/api/v1/shops/{shop_id}/orders?api_key={api_key}&page_number={page_num}&page_size={page_size}&startDateTime={start_time}'  #&endDateTime={end_time}\n",
    "            print(url_order)\n",
    "            data_order  = requests.get(url_order).json()\n",
    "            # Are there any data left?\n",
    "            if len(data_order['data']) != 0:\n",
    "\n",
    "                # If yes, add them to our Variable 'data_need'. And move on to the next page\n",
    "                data_need_order.extend(data_order['data'])\n",
    "                page_num  = page_num + 1      \n",
    "            else:\n",
    "                # If not, move to the next API\n",
    "                page_num  = 1\n",
    "                # (Get out and move to the next API, start from page 1)\n",
    "                break\n",
    "\n",
    "    # Now we got data_need that contain all the data. Make it a dataframe:\n",
    "\n",
    "    df_order = pd.DataFrame(data_need_order)\n",
    "\n",
    "    # Output\n",
    "    return df_order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835f606c-03d0-46a1-82aa-4fbd9a184b99",
   "metadata": {},
   "source": [
    "## 3 - Transform orders data, from JSON format to Pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17e7085f-2edd-4d1c-a76e-fef65b2ed987",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@task(name                = \"Step 3: Transform orders data\",\n",
    "      retries             = 10, \n",
    "      retry_delay_seconds = 60)\n",
    "def s3_transform_order_data(df):\n",
    "    df = df[[ 'id', 'status_name', 'status', 'status_history', 'inserted_at', 'tags', 'histories', 'cod', \n",
    "             'transfer_money', 'warehouse_info', 'partner', 'shop_id', 'assigning_seller' , 'updated_at', 'items']]\n",
    "    \n",
    "     # Create id_column that contain order_id, shop_id:\n",
    "    #Add 2 columns contain 'S' and 'O'\n",
    "    df = df.assign(S='S', O = 'O')\n",
    "\n",
    "    # Concat 3 columns: Format:   S + shop_id + O + id\n",
    "    df['id_column'] = df['S'] + df['shop_id'].astype(str) + df['O'] + df['id'].astype(str)\n",
    "\n",
    "    # remove duplicate based on id_column column\n",
    "    df = df.drop_duplicates(subset=['id_column'], keep='last')\n",
    "    \n",
    "    # Create Revenue column: Revenue = COD + bank transfer\n",
    "    df['revenue'] = df['cod'] + df['transfer_money']\n",
    "\n",
    "    # ______________________________________________________________________________________________________________________________________________________________________________________________________\n",
    "\n",
    "    # Creat tag column\n",
    "    tag_list = []\n",
    "    for i in df['tags']:\n",
    "        tag = []\n",
    "        for q in i:\n",
    "            if str(q).find(\"'name':\") == -1:\n",
    "                tag.append('')\n",
    "            else:\n",
    "                j = 'Tag: (' + q['name'] + ')'\n",
    "                tag.append(j)\n",
    "        k = ', '.join(tag)\n",
    "        tag_list.append(k)\n",
    "    df['tags'] = tag_list\n",
    "\n",
    "    # ______________________________________________________________________________________________________________________________________________________________________________________________________\n",
    "\n",
    "    # Create Warehouse column\n",
    "    warehouse = []\n",
    "    for i in df['warehouse_info']:\n",
    "        if type(i) == dict and 'name' in i:\n",
    "            warehouse.append(i['name'])\n",
    "        else:\n",
    "            warehouse.append('')\n",
    "    df['warehouse_info'] = warehouse\n",
    "\n",
    "    # ______________________________________________________________________________________________________________________________________________________________________________________________________\n",
    "\n",
    "    # create seller column:\n",
    "    seller = []\n",
    "    for i in df['assigning_seller']:\n",
    "        if type(i) == dict and 'name' in i: \n",
    "            seller.append(i['name'])\n",
    "        else:\n",
    "            seller.append('')\n",
    "\n",
    "    df['assigning_seller'] = seller\n",
    "\n",
    "    # ______________________________________________________________________________________________________________________________________________________________________________________________________\n",
    "\n",
    "    # create first_call name column:\n",
    "    first_call = []\n",
    "    for i in df['histories']:\n",
    "\n",
    "        take_out_the_time = []\n",
    "        for e in i:\n",
    "            #check if there is key 'tags' in value in histories and also has tags 1st call for the first time, if yes, take out the 'updated_time' and add to the list\n",
    "            if 'tags' in e and str(e['tags']['new']).lower().find(\"1st call\") != -1 : \n",
    "                take_out_the_time.append(e['updated_at']) \n",
    "        # now, chec each row: if the row have tag 1st call\n",
    "        if len(take_out_the_time) == 0: \n",
    "            first_call.append('')\n",
    "        else:\n",
    "            first_call.append(take_out_the_time[0])\n",
    "    df['first_call'] = first_call\n",
    "\n",
    "    # ______________________________________________________________________________________________________________________________________________________________________________________________________\n",
    "\n",
    "    # create courier partner column\n",
    "    partner = []\n",
    "    for i in df['partner']:\n",
    "        if str(i).find('extend_code') != -1 and str(i).find('order_number_vtp') != -1: \n",
    "            j = str(i['extend_code']) + ' ' + str(i['order_number_vtp'])\n",
    "            partner.append(j)\n",
    "        else:\n",
    "            partner.append('')\n",
    "    df['partner'] = partner\n",
    "\n",
    "    # ______________________________________________________________________________________________________________________________________________________________________________________________________\n",
    "\n",
    "    # tạo 1 new column và sau đó append các giá trị tạo ra sau khi loop vào\n",
    "    # https://www.geeksforgeeks.org/create-a-column-using-for-loop-in-pandas-dataframe/\n",
    "    # Add Waiting for delivery\n",
    "    waiting_delivery_time = []\n",
    "    for h in df['status_history']:\n",
    "        # ghep thêm hàm if để chỉ lấy thời điểm chốt cho các đơn đã chốt\n",
    "        # https://stackoverflow.com/questions/3437059/does-python-have-a-string-contains-substring-method\n",
    "        if str(h).find(\"'status': 9,\")     == -1 and str(h).find(\"'status': 8,\") == -1:\n",
    "            waiting_delivery_time.append('')\n",
    "\n",
    "        elif str(h).find(\"'status': 9,\")   != -1 and str(h).find(\"'status': 8,\") == -1:\n",
    "            waiting_delivery_time.append(str(h).split(\"'status': 9,\", 10)[-1][16:35])\n",
    "\n",
    "        elif str(h).find(\"'status': 9,\")   == -1 and str(h).find(\"'status': 8,\") != -1:\n",
    "            waiting_delivery_time.append(str(h).split(\"'status': 8,\", 10)[-1][16:35])\n",
    "\n",
    "        elif str(h).find(\"'status': 9,\")   != -1 and str(h).find(\"'status': 8,\") != -1:\n",
    "            waiting_delivery_time.append(str(h).split(\"'status': 9,\", 10)[-1][16:35])   \n",
    "\n",
    "        else:\n",
    "            waiting_delivery_time.append('')\n",
    "    df['waiting_delivery_time'] = waiting_delivery_time\n",
    "\n",
    "    # ______________________________________________________________________________________________________________________________________________________________________________________________________\n",
    "\n",
    "    # Add shipped column\n",
    "    shipped_time = []\n",
    "    for h in df['status_history']:\n",
    "        # https://stackoverflow.com/questions/3437059/does-python-have-a-string-contains-substring-method\n",
    "        if str(h).find(\"'status': 2,\") == -1:\n",
    "            shipped_time.append('2010-01-01T00:00:00') # To fix this problem when insert null value into postgres timestamp column: is of type timestamp without time zone but expression is of type double precision\n",
    "        else:\n",
    "            shipped_time.append(str(h).split(\"'status': 2,\", 10)[-1][16:35]) \n",
    "    df['shipped_time'] = shipped_time\n",
    "\n",
    "    # ______________________________________________________________________________________________________________________________________________________________________________________________________\n",
    "\n",
    "    # Add comfirmed time column:\n",
    "    confirmed_time = []\n",
    "    for h in df['status_history']:\n",
    "        # https://stackoverflow.com/questions/3437059/does-python-have-a-string-contains-substring-method\n",
    "        if str(h).find(\"'status': 1,\") == -1:\n",
    "            confirmed_time.append('') \n",
    "        else:\n",
    "            confirmed_time.append(str(h).split(\"'status': 1,\", 10)[-1][16:35]) \n",
    "    df['confirmed_time'] = confirmed_time\n",
    "\n",
    "\n",
    "    # timeline status:\n",
    "    status_history = []\n",
    "    for i in df['status_history']:\n",
    "        if str(i).find(\"'status':\") == -1:\n",
    "            status_history.append('')\n",
    "\n",
    "        else:\n",
    "            status = []\n",
    "            for j in i:\n",
    "                a  = j['updated_at'].split('T', 1)[0] +' '+ j['updated_at'].split('T', 1)[1]\n",
    "                updated_at = pd.to_datetime(a[:19]) + pd.DateOffset(hours=7)\n",
    "                k  = 'S: ' + str(j['status']) + ' - ' + str(updated_at)\n",
    "                status.append(k)\n",
    "            z = ', '.join(status)\n",
    "            if len(z) < 1000:\n",
    "                status_history.append(z)\n",
    "            else:\n",
    "                status_history.append('')\n",
    "    df['status_history'] = status_history\n",
    "\n",
    "    # ______________________________________________________________________________________________________________________________________________________________________________________________________\n",
    "\n",
    "    # convert time:\n",
    "    df['inserted_at']                = convert_time(df['inserted_at'])\n",
    "    df['confirmed_time']             = convert_time(df['confirmed_time'])\n",
    "    df['waiting_delivery_time']      = convert_time(df['waiting_delivery_time'])\n",
    "    df['shipped_time']               = convert_time(df['shipped_time'])\n",
    "    df['first_call']                 = convert_time(df['first_call'])\n",
    "    df['updated_at']                 = convert_time(df['updated_at'])\n",
    "\n",
    "    # ______________________________________________________________________________________________________________________________________________________________________________________________________\n",
    "\n",
    "    # Calculate duration:\n",
    "    df['first_call_lead_time']     = duration(df['first_call'],      df['inserted_at'],             'hour')\n",
    "    df['confirmed_lead_time']      = duration(df['confirmed_time'],  df['inserted_at'],             'hour')\n",
    "    df['warehouse_lead_time']      = duration(df['shipped_time'],    df['waiting_delivery_time'],   'hour')\n",
    "\n",
    "\n",
    "    # Delete no need columns:\n",
    "    # https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html\n",
    "    df_orders = df.drop(columns = ['S', 'O', 'first_call', 'confirmed_time', \n",
    "                             'cod', 'transfer_money', \n",
    "                             'waiting_delivery_time', 'histories'])\n",
    "\n",
    "    # # Use this if you want to update only new event\n",
    "    # https://stackoverflow.com/questions/6871016/adding-days-to-a-date-in-python\n",
    "    # https://www.programiz.com/python-programming/datetime/current-datetime ('Y' and 'y' will give you different year)\n",
    "    today      = date.today().strftime(\"%Y-%m-%d\")\n",
    "    yesterday  = (date.today() + timedelta(days= -1)).strftime(\"%Y-%m-%d\")\n",
    "    two_days_ago = (date.today() + timedelta(days= -2)).strftime(\"%Y-%m-%d\")\n",
    "    # Subset the dataframe when only update new event:\n",
    "\n",
    "    df_orders = df_orders[\n",
    "        # Convert updated_at into string\n",
    "        # https://stackoverflow.com/questions/22005911/convert-columns-to-string-in-pandas\n",
    "        # https://www.statology.org/pandas-check-if-column-contains-string/\n",
    "        # https://stackoverflow.com/questions/22591174/pandas-multiple-conditions-while-indexing-data-frame-unexpected-behavior\n",
    "            df_orders['updated_at'].astype(str).str.contains(today) | df_orders['updated_at'].astype(str).str.contains(yesterday) | df_orders['updated_at'].astype(str).str.contains(two_days_ago)]\n",
    "    \n",
    "    return df_orders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700dd6ed-0a7f-4db4-96bf-fdec0b956300",
   "metadata": {},
   "source": [
    "## 4 - Load data to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa3e8a42-83d8-4f8b-8982-7dbb28f7c766",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@task(name                = \"Step 4.1: load client list data\",\n",
    "      retries             = 10, \n",
    "      retry_delay_seconds = 60)\n",
    "def s4_load_client_list(df):\n",
    "    def insert_into_table(curr, shop_id, client_type, api_key, shop_name, country):\n",
    "        insert_into_orderss = (\"\"\"INSERT INTO d_client (shop_id, client_type, api_key, shop_name, country) VALUES( %s, %s, %s, %s, %s);\"\"\")\n",
    "        row_to_insert = (shop_id, client_type, api_key, shop_name, country)\n",
    "        curr.execute(insert_into_orderss, row_to_insert)\n",
    "\n",
    "\n",
    "    def update_row(curr, shop_id, client_type, api_key, shop_name, country):\n",
    "        query = (\"\"\"UPDATE d_client\n",
    "                SET client_type         = %s,\n",
    "                    api_key\t            = %s,\n",
    "                    shop_name           = %s,\n",
    "                    country             = %s\n",
    "                WHERE shop_id           = %s;\"\"\")\n",
    "        curr.execute(query, vars_to_update)\n",
    "\n",
    "\n",
    "    def check_if_orders_exists(curr, shop_id): \n",
    "        query = (\"\"\"SELECT shop_id FROM d_client WHERE shop_id = %s\"\"\")  \n",
    "\n",
    "        curr.execute(query, (shop_id,))\n",
    "        return curr.fetchone() is not None\n",
    "\n",
    "\n",
    "    def append_from_df_to_db(curr,df):\n",
    "        for i, row in df.iterrows():\n",
    "            insert_into_table(curr, row['shop_id'], row['client_type'],row['api_key'], row['shop_name'], row['country'])\n",
    "\n",
    "\n",
    "    def update_db(curr,df):\n",
    "        tmp_df = pd.DataFrame(columns=['shop_id', 'client_type', 'api_key', 'shop_name', 'country'])\n",
    "        for i, row in df.iterrows():\n",
    "            if check_if_orders_exists(curr, row['shop_id']): # If orders already exists then we will update\n",
    "                update_row(curr, row['shop_id'], row['client_type'], row['api_key'], row['shop_name'], row['country'])\n",
    "            else: # The orders doesn't exists so we will add it to a temp df and append it using append_from_df_to_db\n",
    "                # https://stackoverflow.com/questions/71132469/appending-row-to-dataframe-with-concat\n",
    "                tmp_df = pd.concat([tmp_df, pd.DataFrame([row])])\n",
    "\n",
    "        return tmp_df\n",
    "\n",
    "    # In[6]:\n",
    "    conn = connect_to_db(host_name, dbname, port, username, password)\n",
    "    curr = conn.cursor()\n",
    "\n",
    "    new_vid_df = update_db(curr, df)\n",
    "    conn.commit()\n",
    "\n",
    "    append_from_df_to_db(curr, new_vid_df)\n",
    "    conn.commit()\n",
    "    \n",
    "    \n",
    "@task(name                = \"Step 4.2: load order data\",\n",
    "      retries             = 15, \n",
    "      retry_delay_seconds = 60)\n",
    "def s4_load_order_data(df):\n",
    "\n",
    "    def insert_into_table(curr, shop_id, id, status_name, status, tags, inserted_at, id_column, confirmed_lead_time, updated_at, revenue, shipped_time, warehouse_info, status_history, \n",
    "                          first_call_lead_time, partner, assigning_seller, warehouse_lead_time):\n",
    "        insert_into_orderss = (\"\"\"INSERT INTO test_2 (shop_id, id, status_name, status, tags, inserted_at, id_column, confirmed_lead_time, updated_at, revenue, shipped_time, warehouse_info, status_history, \n",
    "        first_call_lead_time, partner, assigning_seller, warehouse_lead_time)\n",
    "        VALUES(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s);\"\"\")\n",
    "        row_to_insert = (shop_id, id, status_name, status, tags, inserted_at, id_column, confirmed_lead_time, updated_at, revenue, shipped_time, warehouse_info, status_history, \n",
    "                         first_call_lead_time, partner, assigning_seller, warehouse_lead_time)\n",
    "        curr.execute(insert_into_orderss, row_to_insert)\n",
    "\n",
    "\n",
    "    def update_row(curr, shop_id, id, status_name, status, tags, inserted_at, id_column, confirmed_lead_time, updated_at, revenue, shipped_time, warehouse_info, status_history, \n",
    "                   first_call_lead_time, partner, assigning_seller, warehouse_lead_time):\n",
    "        query = (\"\"\"UPDATE test_2\n",
    "                SET shop_id                 = %s,\n",
    "                    id\t                    = %s,\n",
    "                    status_name             = %s,\n",
    "                    status                  = %s,\n",
    "                    tags                    = %s,\n",
    "                    inserted_at             = %s,\n",
    "                    confirmed_lead_time     = %s,\n",
    "                    updated_at              = %s, \n",
    "                    revenue                 = %s,\n",
    "                    shipped_time            = %s,\n",
    "                    warehouse_info          = %s,\n",
    "                    status_history          = %s,\n",
    "                    first_call_lead_time    = %s,\n",
    "                    partner                 = %s,\n",
    "                    assigning_seller        = %s,\n",
    "                    warehouse_lead_time     = %s\n",
    "                WHERE id_column             = %s;\"\"\")\n",
    "        vars_to_update = (shop_id, id, status_name, status, tags, inserted_at, confirmed_lead_time, updated_at, revenue, shipped_time, warehouse_info, status_history, \n",
    "                          first_call_lead_time, partner, assigning_seller, warehouse_lead_time, id_column)\n",
    "        curr.execute(query, vars_to_update)\n",
    "\n",
    "\n",
    "    def check_if_orders_exists(curr, id_column): \n",
    "        query = (\"\"\"SELECT id_column FROM test_2 WHERE id_column = %s\"\"\")\n",
    "\n",
    "        curr.execute(query, (id_column,))\n",
    "        return curr.fetchone() is not None\n",
    "\n",
    "\n",
    "    def append_from_df_to_db(curr,df):\n",
    "        for i, row in df.iterrows():\n",
    "            insert_into_table(curr, row['shop_id'], row['id'], row['status_name'], row['status'], row['tags'], row['inserted_at'], row['id_column'],  row['confirmed_lead_time'], row['updated_at'], row['revenue'], row['shipped_time'], row['warehouse_info'], row['status_history'], \n",
    "                              row['first_call_lead_time'], row['partner'], row['assigning_seller'], row['warehouse_lead_time'])\n",
    "\n",
    "\n",
    "    def update_db(curr,df):\n",
    "        tmp_df = pd.DataFrame(columns=['shop_id', 'id', 'status_name', 'status', 'tags', 'inserted_at', 'id_column', 'confirmed_lead_time', 'updated_at', 'revenue', 'shipped_time', 'warehouse_info', 'status_history', \n",
    "                                       'first_call_lead_time', 'partner', 'assigning_seller', 'warehouse_lead_time'])\n",
    "        for i, row in df.iterrows():\n",
    "            if check_if_orders_exists(curr, row['id_column']): # If orders already exists then we will update\n",
    "                update_row(curr, row['shop_id'], row['id'], row['status_name'], row['status'], row['tags'], row['inserted_at'], row['id_column'],  row['confirmed_lead_time'], row['updated_at'], row['revenue'], row['shipped_time'], row['warehouse_info'], row['status_history'], \n",
    "                           row['first_call_lead_time'], row['partner'], row['assigning_seller'], row['warehouse_lead_time'])\n",
    "            else: # The orders doesn't exists so we will add it to a temp df and append it using append_from_df_to_db\n",
    "                # https://stackoverflow.com/questions/71132469/appending-row-to-dataframe-with-concat\n",
    "                tmp_df = pd.concat([tmp_df, pd.DataFrame([row])])\n",
    "\n",
    "        return tmp_df\n",
    "\n",
    "    #____________________________________________________________________________________________________________________________________________________________________________________________________________\n",
    "    global number_of_chunk\n",
    "    global chunk_list # numpy array\n",
    "    # # Divide data into small chunk, upload them\n",
    "    dfs = np.array_split(df, int(number_of_chunk) +1) # split the dataframe into 161 separate tables\n",
    "\n",
    "    print('- Number of pieces in df is: ' + str(len(dfs)))\n",
    "    print('- Number of Chunk is: ' + str(len(chunk_list)))\n",
    "\n",
    "    # In[6]:\n",
    "\n",
    "    conn = connect_to_db(host_name, dbname, port, username, password)\n",
    "    curr = conn.cursor()\n",
    "    \n",
    "    if len(chunk_list) == 0:\n",
    "        print('Congratulation, sir!')\n",
    "    else:\n",
    "        for i in chunk_list['Chunk']:\n",
    "            print('Uploading _ _ _ _ _ _ _ _ ' + str(chunk_list['Chunk'][i]))\n",
    "            new_vid_df = update_db(curr, dfs[i])\n",
    "            conn.commit()\n",
    "\n",
    "            append_from_df_to_db(curr, new_vid_df)\n",
    "            conn.commit()\n",
    "\n",
    "            print('Drop '+ str(chunk_list['Chunk'][i]))\n",
    "            chunk_list = chunk_list.drop(chunk_list['Chunk'][i])\n",
    "            print('- Number of Chunk left is: ' + str(len(chunk_list)))\n",
    "            print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dbba4c-9b36-499b-be65-1e0e3c346628",
   "metadata": {},
   "source": [
    "#\n",
    "# IV - Combine the whole flow into 1 function and run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c076ba-44f5-4866-88d7-a4f2e0d8dd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "@flow(name                = 'Main pipeline\",\n",
    "      retries             = 10, \n",
    "      retry_delay_seconds = 60)\n",
    "def etl_pipeline():\n",
    "    \n",
    "    # Extract data\n",
    "    client_list           = s1_extract_client_list()\n",
    "    df_order              = s2_extract_orders_api_data(client_list)\n",
    "\n",
    "    # Transform data\n",
    "    df_order              = s3_transform_order_data(df_order)\n",
    "    \n",
    "    # Load data to database\n",
    "    s4_load_client_list(client_list)\n",
    "    s4_load_order_data(df_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b0eb13-e9af-4e3f-8381-817944a7637b",
   "metadata": {},
   "outputs": [],
   "source": [
    "etl_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "865d8b2eb28e274047ba64063dfb6a2aabf0dfec4905d304d7a76618dae6fdd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
